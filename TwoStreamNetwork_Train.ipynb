{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T07:49:55.177501Z",
     "start_time": "2019-07-11T07:49:54.059301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s-sato/dataset/UCF101/data_file.csv\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from TwoStreamNetwork.spatial.trainModel import get_model, freeze_all_but_top, freeze_all_but_mid_and_top\n",
    "from TwoStreamNetwork.spatial.trainData import DataSet, get_generators\n",
    "import time\n",
    "import os.path\n",
    "from os import makedirs\n",
    "from DataSetPathCall import UCF101_PathCall\n",
    "\n",
    "ucf101 = UCF101_PathCall()\n",
    "\n",
    "print(ucf101.getDataListPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T07:49:55.845213Z",
     "start_time": "2019-07-11T07:49:55.180825Z"
    }
   },
   "outputs": [],
   "source": [
    "import trainSetting \n",
    "\n",
    "trainSetting.GPU_Limit(0.9)\n",
    "#trainSetting.GPU_LimitAllow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-11T07:49:52.565Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, nb_epoch, generators, callbacks=[]):\n",
    "    train_generator, validation_generator = generators\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=10,\n",
    "        epochs=nb_epoch,\n",
    "        callbacks=callbacks)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-11T07:49:52.587Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_of_snip=5, saved_weights=None,\n",
    "        class_limit=None, image_shape=(224, 224),\n",
    "        load_to_memory=False, batch_size=32, nb_epoch=100, name_str=None):\n",
    "\n",
    "    # Get local time.\n",
    "    time_str = time.strftime(\"%y%m%d%H%M\", time.localtime())\n",
    "\n",
    "    if name_str == None:\n",
    "        name_str = time_str\n",
    "\n",
    "    # Callbacks: Save the model.\n",
    "    directory1 = os.path.join('out', 'checkpoints', name_str)\n",
    "    if not os.path.exists(directory1):\n",
    "        os.makedirs(directory1)\n",
    "        checkpointer = ModelCheckpoint(\n",
    "            filepath=os.path.join(directory1, '{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "            verbose=1,\n",
    "            save_best_only=True)\n",
    "\n",
    "    # Callbacks: TensorBoard\n",
    "    directory2 = os.path.join('out', 'TB', name_str)\n",
    "    if not os.path.exists(directory2):\n",
    "        os.makedirs(directory2)\n",
    "    tb = TensorBoard(log_dir=os.path.join(directory2))\n",
    "\n",
    "    # Callbacks: Early stoper\n",
    "    early_stopper = EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "    # Callbacks: Save results.\n",
    "    directory3 = os.path.join('out', 'logs', name_str)\n",
    "    if not os.path.exists(directory3):\n",
    "        os.makedirs(directory3)\n",
    "    timestamp = time.time()\n",
    "    csv_logger = CSVLogger(os.path.join(directory3, 'training-' + \\\n",
    "        str(timestamp) + '.log'))\n",
    "\n",
    "    print(\"class_limit = \", class_limit)\n",
    "\n",
    "    if image_shape is None:\n",
    "        data = DataSet(\n",
    "                class_limit=class_limit,\n",
    "                data_path=ucf101.getDataListPath()\n",
    "                )\n",
    "    else:\n",
    "        data = DataSet(\n",
    "                image_shape=image_shape,\n",
    "                class_limit=class_limit,\n",
    "                data_path=ucf101.getDataListPath()\n",
    "                )\n",
    "    \n",
    "    # Get generators.\n",
    "    generators = get_generators(data=data, image_shape=image_shape, batch_size=batch_size\n",
    "                                ,trainDir=ucf101.getTrainDir(), validDir=ucf101.getTestDir())\n",
    "\n",
    "    # Get the model.\n",
    "    model = get_model(data=data)\n",
    "\n",
    "    if saved_weights is None:\n",
    "        print(\"Loading network from ImageNet weights.\")\n",
    "        print(\"Get and train the top layers...\")\n",
    "        model = freeze_all_but_top(model)\n",
    "        model = train_model(model, 10, generators)\n",
    "    else:\n",
    "        print(\"Loading saved model: %s.\" % saved_weights)\n",
    "        model.load_weights(saved_weights)\n",
    "\n",
    "    print(\"Get and train the mid layers...\")\n",
    "    model = freeze_all_but_mid_and_top(model)\n",
    "    model = train_model(model, 10, generators, [tb, early_stopper, csv_logger, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-11T07:49:52.619Z"
    }
   },
   "outputs": [],
   "source": [
    "saved_weights = None\n",
    "class_limit = None  # int, can be 1-101 or None\n",
    "num_of_snip = 1 # number of chunks used for each video\n",
    "image_shape=(224, 224)\n",
    "load_to_memory = False  # pre-load the sequencea in,o memory\n",
    "batch_size = 512\n",
    "nb_epoch = 500\n",
    "name_str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-11T07:49:52.631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_limit =  None\n",
      "Found 1788425 images belonging to 101 classes.\n",
      "Found 697865 images belonging to 101 classes.\n",
      "WARNING:tensorflow:From /home/s-sato/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loading network from ImageNet weights.\n",
      "Get and train the top layers...\n",
      "WARNING:tensorflow:From /home/s-sato/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "  4/100 [>.............................] - ETA: 36:47 - loss: 6.8418 - acc: 0.0195"
     ]
    }
   ],
   "source": [
    "train(num_of_snip=num_of_snip, saved_weights=saved_weights,\n",
    "            class_limit=class_limit, image_shape=image_shape,\n",
    "            load_to_memory=load_to_memory, batch_size=batch_size,\n",
    "            nb_epoch=nb_epoch, name_str=name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
